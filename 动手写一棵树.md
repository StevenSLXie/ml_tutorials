## 动手写一棵树

### 简单的`if-else`

决策树是简单的概念：最简单的决策树，就相当于一个`if-else`，比如下面这个：

                 Feature > 0.5?
                  /          \
               Yes            No
              /                 \
         Class A              Class B


而稍复杂的决策树，也无非是多级`if-else`的叠加
                
                
                       Age > 30?
                      /        \
                   Yes          No
                  /              \
          Income > $50K     Income > $100K
            /        \           /         \
          Yes       No        Yes         No
          /           \        /            \
     Class A    Class B  Class C         Class D
     
真正值得问的问题，是给定一堆特征和标签，我们如何训练一棵决策树？如何利用特征之间的`if-else`，来正确地将标签归类。今天这篇文章，就详细地捋一捋这个话题

### 基于经验的树

考虑这样一个问题：现在有很多水果，对于每个水果，我们只知道它的`直径`、`颜色`，需要根据这两个特征，判断每个水果最可能是哪一种水果（假设总共有四种水果：香蕉、西瓜、苹果、葡萄）

根据实际的生活经验，我们大概可以构造出下面的决策树：

 - 如果直径既大于5厘米又是黄色，则是香蕉
 - 如果直径大于5厘米但是不是黄色，则是西瓜
 - 如果直径小于5厘米且是红色，则是苹果
 - 如果直径小于5厘米但不是红色，则是葡萄
```
               Diameter > 5.0 cm?
                /               \
             Yes                No
             /                    \
      Color is Yellow          Color is Red
      /          \             /          \
     Yes          No        Yes             No
    /              \        /               \
  Banana    Watermelon     Apple            Grape
```  
这棵树非常简单，但我们可以问一些问题：为什么直径的门限是5厘米，而不是6厘米？如果用6厘米，分类是不是会更准确？为什么是先用直径作为第一层分类后用颜色作为第二层分类，如果反过来效果会怎么样？在树的第二层，为什么是用黄色和红色作为分叉的条件，而不是其他颜色？

更进一步说，如果我们的特征不只是直径和颜色，而是报过了重量、色泽、保质期等等，我们还能不能简单的基于经验，构造出一颗能准确分类的决策树？如果我们要分类的不只四种水果，而是几十上百种，那么大概没有人能手动写出一个靠谱的决策树

换句话说，我们需要抛开经验，去寻找一种能自动生成决策树的方法

### 最好的特征，最合适的门限

构造决策树的本质，实际上是要在每一个树的分叉，找一个最好的特征，以及最合适的门限

最好和最合适，其验收标准是当我们用这个特征和门限去分叉之后，不同的标签能够尽可能地被分开

以上面的例子看，通过直径是否大于5厘米，我们基本上把四种水果两两分开了：长的多半是香蕉或西瓜，短的则是苹果和葡萄

从数学上看，分叉之前，四种标签混合在一起，一片混沌。用了直径=5厘米这个条件后，四个标签基本被两两分开了，“混乱”程度开始降低。作为一个分类器，这个决策树开始有点效果了

我们通常用熵来刻画这种混乱度

$$ H(x) = -\sum_{i = 1}^n P(x_i) log(P(x_i)) $$

一开始，我们假设四种水果的数量一样多，那么初始时还没有决策树的时候，其“混乱”程度是

$$ H(x) = -(\sum_{i = 1}^4 0.25 * log(0.25)) = 2 $$ 

分叉之后，每一边只剩下两种水果（我们假设这里分类绝对正确，比如说根据直径>5厘米，符合这个条件的只有香蕉和西瓜，不混杂苹果等），那么其“混乱”程度就变为

$$ H(x) = -(\sum_{i = 1}^2 0.5 * log(0.5)) = 1 $$

混乱程度有所降低，说明这个分叉大体靠谱，至少是有正收益的

如果再以颜色来分的话，理想状态下每个分叉只剩下一种水果，此时根据公式

$$ H(x) = -(\sum_{i = 1}^1 1 * log(1)) = 0 $$

非常纯净

当然，上面是一个理想化的推演，但大体意思在这里：决策树的构造原则，就是要一步步地降低每一片“树叶”的混乱程度。最理想的情况下，每片树叶最后只能有一个标签

明白了这个过程，我们实际上已经可以写出构造决策树的核心代码了。简单来说就是以下几点：

- 对每个特征的每个可能取值都进行遍历
- 从中找出能够使得“混乱”程度降得太大的特征和相应的门限，开始分叉
- 一直分叉直到无法再细分为止

### 来点代码








