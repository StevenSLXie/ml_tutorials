## 动手写一棵树

### 简单的`if-else`

决策树是简单的概念：最简单的决策树，就相当于一个`if-else`，比如下面这个：

                 Feature > 0.5?
                  /          \
               Yes            No
              /                 \
         Class A              Class B


而稍复杂的决策树，也无非是多级`if-else`的叠加
                
                
                       Age > 30?
                      /        \
                   Yes          No
                  /              \
          Income > $50K     Income > $100K
            /        \           /         \
          Yes       No        Yes         No
          /           \        /            \
     Class A    Class B  Class C         Class D
     
真正值得问的问题，是给定一堆特征和标签，我们如何训练一棵决策树？如何利用特征之间的`if-else`，来正确地将标签归类。今天这篇文章，就详细地捋一捋这个话题

### 基于经验的树

考虑这样一个问题：现在有很多水果，对于每个水果，我们只知道它的`直径`、`颜色`，需要根据这两个特征，判断每个水果最可能是哪一种水果（假设总共有四种水果：香蕉、西瓜、苹果、葡萄）

根据实际的生活经验，我们大概可以构造出下面的决策树：

 - 如果直径既大于5厘米又是黄色，则是香蕉
 - 如果直径大于5厘米但是不是黄色，则是西瓜
 - 如果直径小于5厘米且是红色，则是苹果
 - 如果直径小于5厘米但不是红色，则是葡萄
```
               Diameter > 5.0 cm?
                /               \
             Yes                No
             /                    \
      Color is Yellow          Color is Red
      /          \             /          \
     Yes          No        Yes             No
    /              \        /               \
  Banana    Watermelon     Apple            Grape
```  
这棵树非常简单，但我们可以问一些问题：为什么直径的门限是5厘米，而不是6厘米？如果用6厘米，分类是不是会更准确？为什么是先用直径作为第一层分类后用颜色作为第二层分类，如果反过来效果会怎么样？在树的第二层，为什么是用黄色和红色作为分叉的条件，而不是其他颜色？

更进一步说，如果我们的特征不只是直径和颜色，而是报过了重量、色泽、保质期等等，我们还能不能简单的基于经验，构造出一颗能准确分类的决策树？如果我们要分类的不只四种水果，而是几十上百种，那么大概没有人能手动写出一个靠谱的决策树

换句话说，我们需要抛开经验，去寻找一种能自动生成决策树的方法

### 最好的特征，最合适的门限

构造决策树的本质，实际上是要在每一个树的分叉，找一个最好的特征，以及最合适的门限

最好和最合适，其验收标准是当我们用这个特征和门限去分叉之后，不同的标签能够尽可能地被分开

以上面的例子看，通过直径是否大于5厘米，我们基本上把四种水果两两分开了：长的多半是香蕉或西瓜，短的则是苹果和葡萄

从数学上看，分叉之前，四种标签混合在一起，一片混沌。用了直径=5厘米这个条件后，四个标签基本被两两分开了，“混乱”程度开始降低。作为一个分类器，这个决策树开始有点效果了

我们通常用熵来刻画这种混乱度

$$ H(x) = -\sum_{i = 1}^n P(x_i) log(P(x_i)) $$

一开始，我们假设四种水果的数量一样多，那么初始时还没有决策树的时候，其“混乱”程度是

$$ H(x) = -(\sum_{i = 1}^4 0.25 * log(0.25)) = 2 $$ 

分叉之后，每一边只剩下两种水果（我们假设这里分类绝对正确，比如说根据直径>5厘米，符合这个条件的只有香蕉和西瓜，不混杂苹果等），那么其“混乱”程度就变为

$$ H(x) = -(\sum_{i = 1}^2 0.5 * log(0.5)) = 1 $$

混乱程度有所降低，说明这个分叉大体靠谱，至少是有正收益的

如果再以颜色来分的话，理想状态下每个分叉只剩下一种水果，此时根据公式

$$ H(x) = -(\sum_{i = 1}^1 1 * log(1)) = 0 $$

非常纯净

当然，上面是一个理想化的推演，但大体意思在这里：决策树的构造原则，就是要一步步地降低每一片“树叶”的混乱程度。最理想的情况下，每片树叶最后只能有一个标签

明白了这个过程，我们实际上已经可以写出构造决策树的核心代码了。简单来说就是以下几点：

- 对每个特征的每个可能取值都进行遍历
- 从中找出能够使得“混乱”程度降得太大的特征和相应的门限，开始分叉
- 一直分叉直到无法再细分为止

### 来点代码

按照上面的思路，我们可以尝试用代码来实现决策树的逻辑

学过数据结构的都知道，树是一系列节点的结合，我们首先得实现一个基本的节点

```python
class Node:
    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):
        self.feature = feature
        self.threshold = threshold
        self.left = left
        self.right = right
        self.value = value
```

决策树的节点其实分为两类：一种是所谓`树枝`。树枝不是最终的节点，不返回具体的标签（水果种类），只提供了一个判断（比如直径是否大于5厘米），然后根据判断的结果决定如何往下走。另一种是所谓`树叶`，树叶是终端节点，直接返回一个标签。在我们的类设计里面，feature、threshold对应着`树枝`，根据条件判断的结果，可能往left走，也可能往right走。而value是树叶特有的，表示对应的具体标签。

我们可以增加一个函数，`is_leaf()`，来区分一个节点到底是树枝还是树叶。判别的方法也很简单：有value的就是树叶

```python
def is_leaf(self):
   return self.value is not None
```

基于Node，我们可以再创建一个DecisionTree的类。抽象地说，这个类应该包括两个函数：训练fit和预测predict

```python
class DecisionTree:
  def fit(self, X, y):
    pass
  
  def predict(self, X):
    pass
```

预测先不管，我们先来看训练。训练决策树的核心，我们在上面已经研究过了

- 对每个特征的每个可能取值都进行遍历
- 从中找出能够使得“混乱”程度降得太大的特征和相应的门限，开始分叉
- 一直分叉直到无法再细分为止

为了实现这个思路，我们先得写一个辅助函数，来计算熵（“混乱”程度）

$$ H(x) = -\sum_{i = 1}^n P(x_i) log(P(x_i)) $$

```python
def _entropy(self, y):
    prop = np.bincount(y)/len(y)
    return -np.sum([p*np.log2(p) for p in prop if p > 0])
```

我们还需要一个辅助函数，给定一个feature和threshold，我们得能返回哪些样本落在了左边，哪些落在了右边

```python
def _create_split(self, X, thresh):
    left_index = np.argwhere(X <= thresh).flatten()
    right_index = np.argwhere(X > thresh).flatten()
    return left_index, right_index
 ```
 
 有了这两个，我们就可以评估分叉之前和之后的“混乱”程度分别是多少。理论上，决策树每新生成一个枝，“混乱”程度应该有所降低才对
 
 ```python
 def _information_gain(self, X, y, thresh):
    parent_loss = self._entropy(y)
    left_index, right_index = self._create_split(X, thresh)
    n, n_left, n_right = [len(i) for i in [y, left_index, right_index]]

    if n_left == 0 or n_right == 0:
        return 0
    child_loss = (n_left/n) * self._entropy(y[left_index]) + (n_right/n) * self._entropy(y[right_index])
    return parent_loss - child_loss
```

以水果的例子为例，当决策树只有一个根节点，所有水果都没有分开，此时我们会计算其“混乱程度”，这是代码里面的parent_loss。当我们选定一个feature和一个threshold之后，决策树新建出两个分支，直径大于5厘米的去到了左边，小于的去到了右边，此时虽然水果种类没有完全分开，但基本上左边或者右边的树，都只包含了两种水果，每一堆的“混乱程度”是有所降低的。这里我们会分别计算左边和右边的“混乱”程度，并将其加权平均。最后我们返回分叉前后的混乱程度的对比

请注意，上面的讨论里，我们只说了“选定一个feature和一个threshold之后”怎么计算熵的变化，但最关键的“如何选定feature和threshold”还没有涉及。不过有了上面的基础，这个问题也不难：我们可以遍历每一个feature的每一个可能threshold，从中找出能最大程度减少“混乱”的

```python
def _best_split(self, X, y, features):
    split = {'score': -1, 'feat': None, 'thresh': None}

    for f in features:
        X_feat = X[:, f]
        feature_value = np.unique(X_feat)
        for value in feature_value:
            gain = self._information_gain(X_feat, y, value)
            if gain > split['score']:
                split['score'] = gain
                split['feat'] = f
                split['thresh'] = value
    return split['feat'], split['thresh']
```

至此，构造决策树的核心部分，就已经完成了。剩下的部分，就是用递归的方式，不断分叉：

```python
def _build_tree(self, X, y, depth=0):
    self.n_samples, self.n_features = X.shape
    print('n_samples: %d, n_features: %d' % (self.n_samples, self.n_features))
    self.n_class_labels = len(np.unique(y))

    if self._is_finished(depth):
        most_common_Label = np.argmax(np.bincount(y))
        return Node(value=most_common_Label)

    rnd_feats = np.random.choice(self.n_features, self.n_features, replace=False)
    best_feat, best_thresh = self._best_split(X, y, rnd_feats)
    print('at depth %d, best feature=%s, best_threshold=%s' % (depth, best_feat, best_thresh))
    left_index, right_index = self._create_split(X[:, best_feat], best_thresh)

    left_child = self._build_tree(X[left_index, :], y[left_index], depth + 1)
    right_child = self._build_tree(X[right_index, :], y[right_index], depth + 1)
    return Node(feature=best_feat, threshold=best_thresh, left=left_child, right=right_child)
```

基本的思路，就是从根节点不断裂变，但这里还有一个未解的问题：裂变到几时为止？从上面的函数我们看到有个`_is_finished()`,但我们还没有实现这个函数。一般来说，决策树会有两个参数来确保分叉能在某个节点停下来：

- 最大深度max_depth：如果树的深度已经达到max_depth，就不再往下分叉
- 最小分叉数min_sample_split：如果到达该节点的样本数少于min_sample_split，也不再往下分叉

当然，除此之外，还有一个蕴含条件：如果在某个节点，所有样本都同属一个标签，那也就不再往下走了。所以`_is_finished()`可以这样实现：

```python
  def _is_finished(self, depth):
      return depth >= self.max_depth or self.n_class_labels == 1 or self.n_samples <= self.min_samples_split
```

如果不再往下走了，我们应该做什么？前面说过，终端节点是所谓树叶，树叶没有左右子节点，而直接输出标签。一旦到了终端节点，我们就会输出一个标签。这个标签是什么呢？如果到达这个节点的样本都同属于一个标签，那问题就简单了，直接输出该标签即可。但如果样本里还混杂着多个标签，我们就只能输出比例最高的那个

### 预测

理解了训练之后，`predict`就非常简单了，我们从根节点开始，一步一步往下遍历，直到找到终端节点，并输出对应的标签

```python
def _traverse_tree(self, x, node):
    if node.is_leaf():
        return node.value

    if x[node.feature] <= node.threshold:
        return self._traverse_tree(x, node.left)
    else:
        return self._traverse_tree(x, node.right)

def predict(self, X):
    predictions = [self._traverse_tree(x, self.root) for x in X]
    return np.array(predictions)
```
























